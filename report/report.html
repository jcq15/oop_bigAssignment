<!DOCTYPE html>
<html>
<head>
<title>report</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">
/* GitHub stylesheet for MarkdownPad (http://markdownpad.com) */
/* Author: Nicolas Hery - http://nicolashery.com */
/* Version: b13fe65ca28d2e568c6ed5d7f06581183df8f2ff */
/* Source: https://github.com/nicolahery/markdownpad-github */

/* RESET
=============================================================================*/

html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, img, ins, kbd, q, s, samp, small, strike, strong, sub, sup, tt, var, b, u, i, center, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td, article, aside, canvas, details, embed, figure, figcaption, footer, header, hgroup, menu, nav, output, ruby, section, summary, time, mark, audio, video {
  margin: 0;
  padding: 0;
  border: 0;
}

/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

body>*:first-child {
  margin-top: 0 !important;
}

body>*:last-child {
  margin-bottom: 0 !important;
}

/* BLOCKS
=============================================================================*/

p, blockquote, ul, ol, dl, table, pre {
  margin: 15px 0;
}

/* HEADERS
=============================================================================*/

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
}

h1 tt, h1 code, h2 tt, h2 code, h3 tt, h3 code, h4 tt, h4 code, h5 tt, h5 code, h6 tt, h6 code {
  font-size: inherit;
}

h1 {
  font-size: 28px;
  color: #000;
}

h2 {
  font-size: 24px;
  border-bottom: 1px solid #ccc;
  color: #000;
}

h3 {
  font-size: 18px;
}

h4 {
  font-size: 16px;
}

h5 {
  font-size: 14px;
}

h6 {
  color: #777;
  font-size: 14px;
}

body>h2:first-child, body>h1:first-child, body>h1:first-child+h2, body>h3:first-child, body>h4:first-child, body>h5:first-child, body>h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1+p, h2+p, h3+p, h4+p, h5+p, h6+p {
  margin-top: 10px;
}

/* LINKS
=============================================================================*/

a {
  color: #4183C4;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

/* LISTS
=============================================================================*/

ul, ol {
  padding-left: 30px;
}

ul li > :first-child, 
ol li > :first-child, 
ul li ul:first-of-type, 
ol li ol:first-of-type, 
ul li ol:first-of-type, 
ol li ul:first-of-type {
  margin-top: 0px;
}

ul ul, ul ol, ol ol, ol ul {
  margin-bottom: 0;
}

dl {
  padding: 0;
}

dl dt {
  font-size: 14px;
  font-weight: bold;
  font-style: italic;
  padding: 0;
  margin: 15px 0 5px;
}

dl dt:first-child {
  padding: 0;
}

dl dt>:first-child {
  margin-top: 0px;
}

dl dt>:last-child {
  margin-bottom: 0px;
}

dl dd {
  margin: 0 0 15px;
  padding: 0 15px;
}

dl dd>:first-child {
  margin-top: 0px;
}

dl dd>:last-child {
  margin-bottom: 0px;
}

/* CODE
=============================================================================*/

pre, code, tt {
  font-size: 12px;
  font-family: Consolas, "Liberation Mono", Courier, monospace;
}

code, tt {
  margin: 0 0px;
  padding: 0px 0px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px;
}

pre>code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent;
}

pre {
  background-color: #f8f8f8;
  border: 1px solid #ccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px;
}

pre code, pre tt {
  background-color: transparent;
  border: none;
}

kbd {
    -moz-border-bottom-colors: none;
    -moz-border-left-colors: none;
    -moz-border-right-colors: none;
    -moz-border-top-colors: none;
    background-color: #DDDDDD;
    background-image: linear-gradient(#F1F1F1, #DDDDDD);
    background-repeat: repeat-x;
    border-color: #DDDDDD #CCCCCC #CCCCCC #DDDDDD;
    border-image: none;
    border-radius: 2px 2px 2px 2px;
    border-style: solid;
    border-width: 1px;
    font-family: "Helvetica Neue",Helvetica,Arial,sans-serif;
    line-height: 10px;
    padding: 1px 4px;
}

/* QUOTES
=============================================================================*/

blockquote {
  border-left: 4px solid #DDD;
  padding: 0 15px;
  color: #777;
}

blockquote>:first-child {
  margin-top: 0px;
}

blockquote>:last-child {
  margin-bottom: 0px;
}

/* HORIZONTAL RULES
=============================================================================*/

hr {
  clear: both;
  margin: 15px 0;
  height: 0px;
  overflow: hidden;
  border: none;
  background: transparent;
  border-bottom: 4px solid #ddd;
  padding: 0;
}

/* TABLES
=============================================================================*/

table th {
  font-weight: bold;
}

table th, table td {
  border: 1px solid #ccc;
  padding: 6px 13px;
}

table tr {
  border-top: 1px solid #ccc;
  background-color: #fff;
}

table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

/* IMAGES
=============================================================================*/

img {
  max-width: 100%
}
</style>
</head>
<body>
<p><font size=3></p>
<h1><center>面向对象程序设计试点项目报告</center></h1>
<h1><center>计算图</center></h1>
<h2>对计算图的理解</h2>
<p>计算图又叫数据流图，是一种通过有向图的形式表述计算过程的方法。计算图的节点分为常数节点、计算节点、特殊节点（输出、修改等）。节点之间的边表示数据的依赖关系。</p>
<ul>
<li>常数节点主要作为计算图的输入，该节点是零入度节点，用于表示一个常数</li>
<li>计算节点表示一个计算过程，例如加法、乘法等，输入为计算的参数，输出是计算结果</li>
<li>特殊节点包括输出、修改等，用于完善功能</li>
</ul>
<p>使用计算图的好处在于可以结构化地表达计算过程，并且便于进行操作，在机器学习领域有重要应用。Tensorflow是对计算图的一个很好的实现。通过图的形式描述计算有许多优点：</p>
<ol>
<li>通过计算图编写的程序不是逐步计算，而是先构建一个整体的图，然后通过一个session来处理，有利于整体优化，提高效率。</li>
<li>便于求导。在神经网络的随机梯度下降求解方法中需要计算梯度，这个过程往往涉及上百万个参数，如果用传统的方法逐个求导将极其复杂，而基于运算图的反向传播算法(back propagation)可以大大提高计算效率。</li>
</ol>
<p>当然缺点也是显而易见的，这种结构编程较为复杂，也给调试增加了难度。</p>
<h2>安装过程</h2>
<p>按照<a href="https://www.tensorflow.org/api_guides/cc/guide">https://www.tensorflow.org/api_guides/cc/guide</a>的说明进行操作。平台为Windows下的Ubuntu子系统：</p>
<p><code>Linux version 4.4.0-43-Microsoft (Microsoft@Microsoft.com) (gcc version 5.4.0 (GCC) ) #1-Microsoft Wed Dec 31 14:42:53 PST 2014</code></p>
<h3>1.在<a href="https://github.com/tensorflow/tensorflow">GitHub仓库</a>中下载源码</h3>
<h3>2.<a href="https://docs.bazel.build/versions/master/install-ubuntu.html#install-with-installer-ubuntu">安装Bazel</a></h3>
<p>按照说明操作即可，不要忘了将Bazel安装目录添加到PATH：</p>
<pre><code>$ sudo vim /etc/profile 
</code></pre>

<p>在末尾加入</p>
<pre><code>export PATH=&quot;$PATH:$HOME/bin&quot;
</code></pre>

<p>上面是默认安装位置，可根据实际情况修改。</p>
<h3>3.配置</h3>
<p>运行<code>./configure</code>进行配置，里面选项我也不知道干嘛的，就全用了默认值，结果出错了。查阅<a href="https://www.zybuluo.com/kakadee/note/657235">资料</a>发现，其中有一步</p>
<pre><code>Do you wish to build TensorFlow with OpenCL support? [y/N]
</code></pre>

<p>要选<code>n</code>，否则会出现如下一直循环的情况：</p>
<p><img src="xunhuan.png" /></p>
<p>如果不幸出现这种情况，请<code>Ctrl+C</code>结束程序，重新配置。</p>
<h3>4.测试</h3>
<p>复制一份源码，建立文件<code>tensorflow/cc/example/example.cc</code>，写入如下内容：</p>
<pre><code>// tensorflow/cc/example/example.cc

#include &quot;tensorflow/cc/client/client_session.h&quot;
#include &quot;tensorflow/cc/ops/standard_ops.h&quot;
#include &quot;tensorflow/core/framework/tensor.h&quot;

int main() {
  using namespace tensorflow;
  using namespace tensorflow::ops;
  Scope root = Scope::NewRootScope();
  // Matrix A = [3 2; -1 0]
  auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });
  // Vector b = [3 5]
  auto b = Const(root, { {3.f, 5.f} });
  // v = Ab^T
  auto v = MatMul(root.WithOpName(&quot;v&quot;), A, b, MatMul::TransposeB(true));
  std::vector&lt;Tensor&gt; outputs;
  ClientSession session(root);
  // Run and fetch v
  TF_CHECK_OK(session.Run({v}, &amp;outputs));
  // Expect outputs[0] == [19; -3]
  LOG(INFO) &lt;&lt; outputs[0].matrix&lt;float&gt;();
  return 0;
}
</code></pre>

<p>建立文件<code>tensorflow/cc/example/BUILD</code>，写入如下内容：</p>
<pre><code>load(&quot;//tensorflow:tensorflow.bzl&quot;, &quot;tf_cc_binary&quot;)

tf_cc_binary(
    name = &quot;example&quot;,
    srcs = [&quot;example.cc&quot;],
    deps = [
        &quot;//tensorflow/cc:cc_ops&quot;,
        &quot;//tensorflow/cc:client_session&quot;,
        &quot;//tensorflow/core:tensorflow&quot;,
    ],
)
</code></pre>

<p>运行<code>bazel run -c opt //tensorflow/cc/example:example</code>，然后见证奇迹即可。首次编译时间较长，可以先去打一局农药（误）。如果输出是<code>19 -3</code>则运行正确。</p>
<h2>类与接口介绍</h2>
<p>以下对常用的类与接口进行简单介绍，具体内容可查阅<a href="https://www.tensorflow.org/api_guides/cc/guide">官方文档</a>。</p>
<p>首先需要说明一些常用概念。在TensorFlow中，用<strong>张量(tensor)</strong>来表示数据，使用<strong>图(graph)<strong>来表示计算任务，图中的节点称为</strong>op(operation)</strong>。计算的过程是，在<strong>会话(Session)</strong>里启动一个图，会话将图的op分发到CPU（或GPU）中计算，然后返回<code>tensorflow::Tensor</code>实例。</p>
<h3>tensorflow::Scope</h3>
<p>Scope类是维护计算图当前状态的主要数据结构，里面包含了计算图的一些属性，也封装了一些TensorFlow的操作。在构造节点时，Scope对象需要作为第一个参数传入。</p>
<pre><code>Scope root = Scope::NewRootScope();
//生成一个新的scope
</code></pre>

<h3>Operation Constructors</h3>
<p>TensorFlow中，不同的op类型由不同的类实现，我们可以通过Operation Constructors来构造节点。所有的Operation Constructors第一个参数都为Scope对象，因此首先需要定义一个Scope。</p>
<pre><code>Scope scope = Scope::NewRootScope();
</code></pre>

<h4>运算节点</h4>
<pre><code>auto a = Add(scope, a, b);                  //加法
auto m1 = Multiply(scope, a, b);            //这里做的是对应元素相乘，注意和下面的矩阵乘法区分
auto m2 = MatMul(scope, a, b);              //创建矩阵乘法节点，a, b为两个输入参数
auto m3 = MatMul(scope, a, b, MatMul::TransposeA(true));
//构造节点时可以指定一些属性，上面表示对第一个参数进行转置
auto m = MatMul(scope, a, b, MatMul::TransposeA(true).TransposeB(true));
//多个属性可以这样写，表示对两个参数都进行转置操作
</code></pre>

<h4>常数节点</h4>
<pre><code>auto f = Const(scope, 3.14);                    //创建浮点常量
auto s = Const(scope, &quot;Helloworld!&quot;);           //字符串常量
auto t = Const(scope, {{{1},{2},{3}}});         //创建1×3×1的张量
auto c1 = Const(scope, 10, /* shape */ {2, 2}); 
//也可以这样指定一个2×2的矩阵
auto c2 = Const(scope, {1, 2, 3, 4, 5, 6}, /* shape */ {1, 3, 2, 1});
//这样则是1×3×2×1的四阶张量，比大括号嵌套的写法可读性更强
</code></pre>

<h4>Placeholder</h4>
<p>Placeholder允许我们在运行时输入节点的数值，而不必在构建运算图时指定数值，从而起到占位符的作用。</p>
<pre><code>auto a = Placeholder(scope, DT_INT32);
</code></pre>

<p>第二个参数表示数据类型，常用数据类型如下表，更多内容请查阅<a href="https://www.tensorflow.org/versions/r1.0/programmers_guide/dims_types">官方API</a>：</p>
<table>
<thead>
<tr>
	<th>数据类型</th>
	<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
	<td>DT_INT32</td>
	<td>32位整数</td>
</tr>
<tr>
	<td>DT_FLOAT</td>
	<td>32位浮点数</td>
</tr>
<tr>
	<td>DT_DOUBLE</td>
	<td>64位浮点数</td>
</tr>
<tr>
	<td>DT_COMPLEX64</td>
	<td>两个float组成的复数</td>
</tr>
<tr>
	<td>DT_STRING</td>
	<td>字符串</td>
</tr>
</tbody>
</table>
<h3>tensorflow::ClientSession</h3>
<p>我们需要一个session来执行运算图。session可以对计算图进行封装。具体地，我们将使用ClientSession类来实现。主要接口如下：</p>
<pre><code>ClientSession(const Scope &amp; scope);
//构造函数，接收一个Scope对象

Run(const std::vector&lt;Output&gt;&amp; fetch_outputs, std::vector&lt;Tensor&gt;* outputs) const;
//运行计算图，第一个参数为要计算的节点们，第二个参数为保存输出的地址

Run(
  const FeedType &amp; inputs,                      //以map形式输入参数
  const std::vector&lt; Output &gt; &amp; fetch_outputs,  //要计算的节点
  std::vector&lt; Tensor &gt; *outputs                //输出
) const;
</code></pre>

<p>下面是一个例子：</p>
<pre><code>Scope root = Scope::NewRootScope();         //搞一个scope
auto c = Const(root, { {1, 1} });           
auto m = MatMul(root, c, { {42}, {1} });    //创建两个节点

ClientSession session(root);                //创建session
std::vector&lt;Tensor&gt; outputs;                //用它保存输出
session.Run({m}, &amp;outputs);
// outputs[0] == {42}
</code></pre>

<h2>构建运算图</h2>
<p>利用上面介绍的接口，我们可以建立一个简单的运算图，如下。</p>
<p><img src="calculate.png" /></p>
<p>abcd为四个输入节点，中间有若干计算节点，最后算出h的值。重写example的代码：</p>
<pre><code>#include &quot;tensorflow/cc/client/client_session.h&quot;
#include &quot;tensorflow/cc/ops/standard_ops.h&quot;
#include &quot;tensorflow/core/framework/tensor.h&quot;

int main(){
    using namespace tensorflow;
    using namespace tensorflow::ops;
    Scope root = Scope::NewRootScope();

    /* 构建运算图 */
    auto a = Placeholder(root, DT_INT32);
    auto b = Placeholder(root, DT_INT32);
    auto c = Const(root, { {3, 1}, {2, 2} });
    auto d = Placeholder(root, DT_INT32);

    auto e = MatMul(root, a, b);
    auto f = Add(root, b, c);
    auto g = MatMul(root, e, f, MatMul::TransposeB(true));
    auto h = Multiply(root, d, g);      //注意这里的乘法是元素直接乘

    /* 执行计算 */
    std::vector&lt;Tensor&gt; outputs;
    ClientSession session(root);
    session.Run(
        { {a, { {2, 1}, {0, 5} }}, {b, { {3, 3}, {7, -2} }}, {d, { {5, 5}, {5, 5} }} },
        //全是大括号……眼花
        {h},
        &amp;outputs);
    LOG(INFO) &lt;&lt; outputs[0].matrix&lt;int&gt;();

    return 0;
}
</code></pre>

<p>再次运行<code>bazel run -c opt //tensorflow/cc/example:example</code>，可以看到输出如下：</p>
<p><img src="output.png" /></p>
<p>输出和预期一致，大成功！</p>
<h2>设计的优劣</h2>
<p>我们不打算从分布式性能、GPU内存占用等比较高级的应用层面进行评价，下面仅从面向对象程序设计课程的角度简单地探讨一下接口的优劣。</p>
<p>通过上面几个小例子可以看出，每一种运算节点都是单独的一个类，这样便于对不同的节点编写具体的实现函数。其优点就是面向对象编程的优点，用户只需调用提供的接口，不必关心内部实现（实际上我找了半天，想看看节点类的继承结构，但没找到）。例如在创建节点的时候，写法很统一：</p>
<pre><code>auto &lt;变量名&gt; = &lt;节点名&gt;(&lt;Scope对象&gt;, &lt;其他参数&gt;);
</code></pre>

<p>不管是计算节点还是常数节点，或者是Placeholder，定义方式都是统一的，便于编程。</p>
<p>通过<code>ClientSession</code>类把图的构造和执行分开，便于分开编程和调试，这也体现了面向对象编程的特点。</p>
<p>说到缺点，主要是难以上手。其他的缺点暂时也想不到什么，毕竟是Google的大佬们研发出来，并且修改了这么多版的。没有研究底层实现，也用的比较少，很难找出什么明显缺点。</p>
<h2>*求导功能</h2>
<h2>*Variable</h2>
<h2>*神经网络手写数字识别</h2>
<p>简要介绍一下神经网络手写数字识别的原理。</p>
<h3>Sigmoid neurons</h3>
<p>Sigmoid neurons就像神经元，有若干输入（即一个向量）和一个输出。输入一个向量<img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=x" alt="x" />，通过一个函数计算出输出<img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=f(x)" alt="f(x)" />。在手写数字识别中，我们可以采用这样的函数：</p>
<p><center><img src="e1.gif" /></center>
其中</p>
<p><center><img src="e3.gif" /></center>
是仿射变换，</p>
<p><center><img src="e2.gif" /></center>可以将输出光滑化，便于后面进行随机梯度下降寻找最优解。</p>
<h3>构建神经网络</h3>
<p>用许多个这种神经元，我们可以构造出一个多层神经网络，如下图。</p>
<p><img src="http://neuralnetworksanddeeplearning.com/images/tikz12.png" /></p>
<p>(图片来源：http://neuralnetworksanddeeplearning.com/chap1.html，仅用作学习使用)</p>
<p>其中最左侧第一层的输入是待识别图片每一个像素的灰度值，中间有15个神经元，具体参数待定，10个输出表示该图片与某个数字的相似程度。</p>
<h3>随机梯度下降</h3>
<p>下一步就是计算神经元的参数，这里采用随机梯度下降法。首先我们需要有一组训练数据，对于数据集中的每一个输入
<img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=x_i" alt="x_i" />
，我们都知道它的期望输出
<img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=a" alt="a_i" />
。先随机取一组初始参数，计算出
<img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=x_i" alt="x_i" />
对应的输出
<img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=y_i" alt="y_i" />
，然后计算误差函数：</p>
<p><center><img src="http://chart.googleapis.com/chart?cht=tx&amp;chl=g(\omega,b)=\frac{1}{n}\sum_{i=1}^n|y_i-x_i|^2" /></center>
误差函数的自变量是神经元的参数，我们只要计算出梯度，然后让参数沿着梯度方向小幅度调整，就可以逐渐使误差趋向最低点，也就找到了最优解。</p>
<p>实际的神经网络中往往需要大量的神经元，因此参数个数巨大，计算梯度较为复杂，如果采用基于计算图的反向传播算法，则可以方便地进行梯度计算。</p>
<h2>References</h2>
<p>以下页面在完成作业时有参考，但未在文中显式列出或加入超链接，在此一并表示感谢。</p>
<ol>
<li><a href="https://blog.csdn.net/jmh1996/article/details/73201060#t0">Tensorflow C++学习(二)</a></li>
<li><a href="https://blog.csdn.net/lenbow/article/details/52152766">Tensorflow一些常用基本概念与函数（1）</a></li>
<li><a href="https://blog.csdn.net/jmh1996/article/details/78091115">为什么Tensorflow需要使用&quot;图计算&quot;来表示计算过程</a></li>
<li><a href="https://github.com/tensorflow/serving/issues/518">Feeding a value for placeholder tensor in Tensorflow C++ API</a></li>
<li><a href="https://blog.csdn.net/JNingWei/article/details/73550444">Tensorflow源码 目录树</a></li>
</ol>

</body>
</html>
<!-- This document was created with MarkdownPad, the Markdown editor for Windows (http://markdownpad.com) -->
